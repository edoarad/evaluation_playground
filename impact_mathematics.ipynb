{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lognormal distributions for cost-effectiveness analysis\n",
    "1. \"everything is lognormal\".\n",
    "2. for $X,Y$ independent, $E[XY] = E[X]E[Y]$ \n",
    "3. for $X,Y$ independent, $Var[X+Y] = Var[X] + Var[Y]$.\n",
    "4. we can think of a lognormal distribution with parameters $\\mu, \\sigma^2$ as a variable centered around $e^{\\mu}$ with a *multiplicative* spread of $e^{\\sigma}$\n",
    "4. I expect this is how most people implicitly think of lognormal distributions when they make estimations.\n",
    "5. for a lognormally distributed $X$, $E[X] = e^{\\mu + \\sigma^2/2}$. In particular, this is different from the median of $X$ which is $e^{\\mu}$ (its \"center\") \n",
    "6. generally, $E[\\frac{1}{X}] \\neq \\frac{1}{E[X]}$\n",
    "7. in particular, even for $X,Y$ independent, $E[\\frac{X}{Y}] \\neq \\frac{E[X]}{E[Y]}$ generally\n",
    "8. for $X\\sim \\text{Lognormal}(\\mu_X, \\sigma^2_X)$, it's reciprocal is $\\frac{1}{X}\\sim \\text{Lognormal}(-\\mu_X, \\sigma^2_X)$\n",
    "9. for $Y\\sim \\text{Lognormal}(\\mu_Y, \\sigma^2_Y)$, $XY$ is lognormally distributed with parameters $\\mu_X + \\mu_Y$ and (if they are independent) $\\sigma^2_X + \\sigma^2_Y$\n",
    "10. in many cases, cost-effectiveness estimates are of the form $\\frac{\\prod X_i}{\\prod Y_j}$, where all variables are assumed to be independently lognormally distributed (maybe there are more multiplicative factors). This can be simplified to a log of a sum of normal distributions. \n",
    "11. it's expected value is $E[\\frac{\\prod X_i}{\\prod Y_j}] = e^{\\sum \\mu_{X_i} - \\sum \\mu_{Y_j} + \\frac{\\sum \\sigma^2_{X_i} + \\sum \\sigma^2_{Y_j}}{2}}$ \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everything is lognormal!(?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central limit theorem (CLT) states roughly that a sum of many independent variables can be approximated by a normal distribution. This implies that a multiplication of many independent random variables tends to a lognormal distribution (because we can take the logarithm of both sides!). This is one criterion for when we should expect to get a lognormal distribution - when the number we are trying to estimate breaks naturally to a product of unknown variables.\n",
    "\n",
    "For example, consider the classical question of \"How many windows are there in New York?\". We can split this as:\n",
    "$$\n",
    "\\text{Amount of windows in New York} = \\text{average amount of windows per building }\\cdot\\text{ number of buildings}\n",
    "$$\n",
    "and we can further break this down\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Amount of windows in New York }    &= \\text{ average building height } \\cdot \\\\\n",
    "    &\\cdot\\text{ average building width }\\cdot \\\\\n",
    "    &\\cdot \\text{ average number of sides of a building }\\cdot\\\\\n",
    "    &\\cdot\\text{ average amount of windows per sq meter} \\cdot\\\\\n",
    "    &\\cdot\\text{ size of New York } \\cdot \\\\\n",
    "    &\\cdot\\text{ percent of the area which is built } \\cdot \\\\\n",
    "    &\\cdot\\text{ number of buildings per area } \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "But this example is a bit misleading. Many parameters here are dependent, such as average building height and average amount of windows per area. Also, there are additional parameters that should be included additively. Say, what about car windows? Also, we could sum the results for different types of buildings, or different areas in the city.\n",
    "\n",
    "Dealing with dependencies can be tricky. One standard approach is to condition on another variable which is causally influencing both. Say, whether the building area is industrial, suburban, centrally located, rich/poor, etc. influences both the average building height and average amount of windows per area and captures a lot of the dependence (at least those I had in mind). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal distributions are symmetric around the mean $\\mu$. This means that we would be similarly surprised to get a result of $\\mu+d$ as we would for $\\mu-d$. Therefore, lognormal distributions are symmetric around $e^\\mu$, it's median, in log-scale. Or, equivalently, we would be just as surprised to find $e^\\mu d$ as we would for $e^\\mu d^{-1}$. That could be another useful rule of thumb."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation Uncertainty\n",
    "\n",
    "When estimating a variable $X$, we have both \"epistemic\" and \"statistical\" (or, \"aleatoric\") uncertainty about its value. Statistical uncertainty can be thought of as the inherent randomness of $X$ (e.g. the number of heads in 10 coin flips). Epistemic uncertainty is uncertainty about the value of $X$ due to our lack of knowledge about it (e.g. the number of people in the world who have ever lived). \n",
    "\n",
    "For example, in a randomized controlled trial (RCT), we try to estimate the effect of a treatment on some outcome. That effect is usually dependent on many particular factors, such as the financial and cultural aspects of the population involved, the time of day the treatment was administered, the prevalence of a specific disease in a particular location, etc. With infinite knowledge, we could account for all of these factors and estimate the effect as a function of them. However, as this isn't possible, we can instead try model the effect as a random variable with some *statistical* uncertainty. \n",
    "\n",
    "If we conduct that RCT well, have a large sample size, and we have drawn randomly from our target population, then we can find a good fit for the distribution of the effect which can then be used to predict the effect of a future large-scale program. In this case, we have very little *epistemic* uncertainty about the effect, but the statistical uncertainty is still present and unreducible.\n",
    "\n",
    "If we would have tried to apply the results of that RCT to a different population, then we would have had to account for the epistemic uncertainty as well. For example, if we had conducted the RCT in a rural area of a developing country, and we wanted to apply the results to an urban area of that same country, then we would have to make some educated guesses about the effect of the treatment in the new population. This is in epistemic uncertainty territory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Example - Water Treatment Interventions\n",
    "In 2022 GiveWell revised upwards their [assessment](https://www.givewell.org/international/technical/programs/water-quality-interventions) of the cost-effectiveness of chlorination interventions to improve water quality in subsaharan Africa, and have [funded](https://www.givewell.org/research/grants/evidence-action-dispensers-for-safe-water-January-2022) Evidence Action's Dispensers for Safe Water program to the amount of $65m. \n",
    "\n",
    "Their analysis was based on a meta-analysis by Kremer et. al. of related RCTs, estimating the effect of chlorination interventions on mortality. It is currently a working paper (the latest version have is [here](https://bfi.uchicago.edu/wp-content/uploads/2022/03/BFI_WP_2022-26.pdf), as of July 2023), which means that they are still performing some analyses an it hasn't yet been peer-reviewed. \n",
    "\n",
    "In GiveWell's analysis, they performed their own estimation, based on adjustments to Kremer et. al.'s meta-analysis.  \n",
    "\n",
    "[I want to show the diamond plot, how GiveWell use it, their criticism from the competition, and Witold's reaction. All of this tells a nice story about how meshing together different types of uncertainties can be tricky. However, I don't feel like I have a good solution, so maybe I should just leave it out.]\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epistemic Elicitation\n",
    "What exactly goes on when we try to guess some parameter? Say, when we estimate the number of people living in rural areas in sub-Saharan Africa, we might think something like \"well, I think I remember somewhere around 30% of the people there that live in rural areas, but I won't be very surprised if it's 10% or 70%. Also, the overall number of people living there is probably about 1 billion, likely off by a factor of 2 or so\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, are we talking about epistemic or statistical uncertainty? One may argue that there is a real, objective, number that expresses this amount of people, and therefore there's no statistical uncertainty involved. I think that's the wrong way to look at it. There are two main reasons to use this epistemic/statistical distinction:\n",
    "1. Variation over time (or over similar instances) is usefully modeled as statistical uncertainty.\n",
    "2. If we can practically narrow the distribution by obtaining accessible and relevant information or performing a feasible analysis, we can usefully model the randomness that results from varying such possible accessible information as epistemic uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, I think it's useful to model the random daily fluctuations and general trends in the context of statistical uncertainty (although we may have epistemic uncertainty around the distributions thereof!). Without looking for further information from trusted sources online, I think most of the uncertainty will be dominated by the epistemic uncertainty I have (in this case). However, it's plausible that if I find some credible sources claiming such and such numbers, I may be confident enough in the results such that the random flunctuations will matter as much or more than my subjective credence variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important aspect of statistical models is their falsifiability or their susceptibility to empirical evaluation. In academic literature most of the focus is on testing models that account for statistical uncertainty. Say, we can suggest a model for population growth where the yearly growth rate changes by adding a normal distribution centered around 0 with a small constant sd to the last year's growth rate. This, then, can be tested against available past data. However, when I say \"I think there's roughly 50% chance that there's at least 1 billion people living in sub-saharan Africa\", how can we test that?\n",
    "\n",
    "One way would be to check calibration over many different questions of a similar nature. That is, whether roughly 50% of the time my cut-off $T$ for \"Location $X$ has $> T$ population\" holds. If this holds for more than 50% of the time, that's not good! It means I should generally choose lower thresholds. There are many intricacies here. For example, if I'm making predictions for such thresholds as either a lower or upper bound, I can cheat and toss a coin to decide whether I want $>T$ or $<T$ and I'll be perfectly calibrated. Another way to cheat would be to alternately choose an unreasonably high and an unreasonably low thresholds, which would also guarantee perfect calibration. \n",
    "\n",
    "Two main problems come to mind:\n",
    "1. How transferrable is the skill of being calibrated? If I practice on estimating population sizes, would that help me be calibrated when estimating, say, average per-capita GDP? Would I be as calibrated in questions where I can quickly search for information vs those I have no idea about?\n",
    "2. Once I've made a prediction, how do I know whether that particular prediction is correct? I said something is 20% likely, how can someone else evaluate whether that particular prediction makes sense, or whether I was lying, say?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first question is empirical, and I'm sure that there is literature on the matter. The second is tricky. Maybe one approach would be to open up the reasoning process, but I'm not really optimistic about that. I think a slightly easier question might be \"how do I know that I'm correct?\", where there's no adversarial setting and as good an access as possible to my own mind.\n",
    "\n",
    "...\n",
    "\n",
    "What I'd also like to get at, is how do we assess which distributions are more correct when describing priors / epistemic uncertainty for a particular estimand. I think this can be reduced to evaluating estimated likelihoods, by considering the cdf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
