{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking for Maximum Impact\n",
    "\n",
    "We have a list of charities, $x_1,\\ldots,x_n$, and a list of expert judges $e_1,\\ldots,e_k$. Each expert judges some charities and gives them a numerical score $e_j(x_i)$. The scores are based on that expert's subjective judgement across a dozen or so criteria, each are scored with a number 1-5 and then are weightedly summed. \n",
    "\n",
    "Our question is: how do we rank the charities? \n",
    "\n",
    "## model\n",
    "\n",
    "Let's assume that each charity $x_i$ has a \"true\" value of $y_i$, which the experts are trying to estimate. \n",
    "\n",
    "The experts are additively and multiplicatively biased, and their scores are noisy. We model this as follows:\n",
    "\n",
    "$$ e_j(x_i) = m_j y_i + b_j + \\varepsilon_{j,i}$$\n",
    "\n",
    "where $m_j$ is the constant multiplicative bias for each expert $e_j$, $b_j$ is the constant additive bias, and $\\varepsilon_{j,i} \\sim \\text{N}(0,\\sigma_j)$ is the noise. All variables are assumed to be independent.\n",
    "\n",
    "## optimization\n",
    "\n",
    "We are looking for a solution that'd be the \"best\" fit. A solution would include all of the $y_i$'s, $m_j$'s, $b_j$'s, and $\\sigma_j$'s. Note that we have a couple of degrees of freedom here, which amount to changing all $y_i$ by a single linear transformation. We could solve that by normalizing the $y_i$ to have mean 0 and variance 1, for example.\n",
    "\n",
    "One option would be to define the best fit as the solution that maximizes the likelihood of the available estimates $e_j(x_i)$. After taking logarithm and dropping the constant terms, we get the following expression we want to maximize:\n",
    "\n",
    "$$ \\sum_{i,j} \\left( -\\frac{1}{2} \\log(2\\pi\\sigma_j^2) - \\frac{1}{2\\sigma_j^2} (e_j(x_i) - m_j y_i - b_j)^2 \\right) $$\n",
    "\n",
    "where the sum is over all charity/expert pair that's available. Equivalently, we can minimize\n",
    "\n",
    "$$ 2\\sum_j \\log(\\sigma_j) + \\sum_{i,j} \\frac{1}{2\\sigma_j^2} (e_j(x_i) - m_j y_i - b_j)^2 $$\n",
    "\n",
    "where the multiplier 2 is the number of charities each expert has (in our case).\n",
    "\n",
    "This is nearly a convex optimization problem, so we can use a standard solver to find the solution.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[$m_jy_i$ isn't convex. Also $1/\\sigma_j^2$ isn't convex]\n",
    "\n",
    "For simplicity, let's assume that all experts have the same error distribution, i.e. $\\sigma_j = \\sigma$ for all $j$. Thus, we get a least squares problem:\n",
    "\n",
    "$$ \\min_{y,m,b} \\sum_{i,j} (e_j(x_i) - m_j y_i - b_j)^2 $$\n",
    "\n",
    "In experiments below, this has problems converging. To solve this, we will estimate $m_j$ in two ways. First we will take it to be 1, and second we will estimate it from the data directly for each expert to normalize their own grades. We expect the truth to be somewhere in between, so we'll see how that affects the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add a regularization term to make sure $b_j$ is small and we normalize the $e_j(x_i)$'s to have mean 0 and variance 1.\n",
    "\n",
    "Now, we can represent the problem as a matrix equation:\n",
    "\n",
    "$$ \\min_{y,b} \\| E - A(y,b) \\|_F^2 + \\lambda \\| b \\|_2^2 $$\n",
    "\n",
    "where $E$ is the matrix of expert scores, $A$ is a $nk,n+k$ matrix taking the concatenation of the vectors $y$ and $b$ to the estimate at each index, and $\\lambda$ is the regularization parameter. The matrix $A$ is defined as follows:\n",
    "\n",
    "$$ A_{(i,j),l} = \\begin{cases} \\delta_{i,l}m_j & \\text{if } l \\leq n \\\\ \\delta_{j,l} & \\text{otherwise} \\end{cases} $$\n",
    "\n",
    "where $\\delta$ is the Kronecker delta (1 if the indices are equal, 0 otherwise). We can then add extra rows to represent the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['org', 'judge', 'grade', 'norm_grade', 'm'], dtype='object')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('./df_grades - Sheet1.csv')\n",
    "df['norm_grade'] = (df.grade - df.grade.mean()) / df.grade.std()\n",
    "df['m'] = 1\n",
    "charities = sorted(df['org'].unique())\n",
    "experts = sorted(df['judge'].unique())\n",
    "n = len(charities)\n",
    "k = len(experts)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix():\n",
    "    a = np.zeros([n*k + k, n+k])\n",
    "    for row in df.iterrows():\n",
    "        i = charities.index(row[1]['org'])\n",
    "        j = experts.index(row[1]['judge'])\n",
    "        m = row[1]['m']\n",
    "        a[i*k + j, i] = m\n",
    "        a[i*k + j, n + j] = 1\n",
    "    for j in range(k):\n",
    "        a[n*k + j, n + j] = 1\n",
    "    return a       \n",
    "\n",
    "def create_target_vector():\n",
    "    e = np.zeros(n*k + k)\n",
    "    \n",
    "    for row in df.iterrows():\n",
    "        i = charities.index(row[1]['org'])\n",
    "        j = experts.index(row[1]['judge'])\n",
    "        e[i*k + j] = row[1]['norm_grade']\n",
    "\n",
    "    # the last k values are 0\n",
    "\n",
    "    return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = create_matrix()\n",
    "e = create_target_vector()\n",
    "sol = np.linalg.lstsq(a, e, rcond=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
