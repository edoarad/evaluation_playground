{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGI Takeoff dynamics - Intelligence vs Quantity explosion\n",
    "## Summary\n",
    "In Galor & Weil's Unified Growth Theory (UGT), a model is given to explain the drivers for economic growth throughout history. The main insight of the model is that until relatively recently it was more worthwhile to have more kids than to invest in their education. This is because the marginal product of labor was low, and the marginal product of education was even lower. However, as technology advanced, the marginal product of labor increased, and the marginal product of education increased even more. This led to a transition from a Malthusian equilibrium to a modern growth equilibrium that's dominated more by technological growth.\n",
    "\n",
    "We explore how this model could be extended to explain AGI takeoff dynamics. Our key question is whether we expect AGI to gain more by investing in improving its intelligence or by copying itself many times. \n",
    "\n",
    "## Model overview\n",
    "We assume that all AI agents have a common goal and make similar decisions, but we don't account for any coordination between them. Their utility is given by a time-discounted variable that depends on efficiency-weighted effort they put directly into it, so we think of it as \"consumption\". They care about their own consumption, but also about the consumption of their descendants.\n",
    "\n",
    "The efficiency of an AI agent is given by its intelligence, which is a function of the amount of effort that went into its training. \n",
    "\n",
    "The resources they can spend are all compute-time, which itself can grow if they put effort into it.\n",
    "\n",
    "When an agent gets access to more compute, it can use it by copying itself and training the copies. They can choose to put no effort into training, in which case the copies will be identical to the original. [TODO maybe it'd be easier if the agent can train itself, rather than train its decendents?]\n",
    "\n",
    "We also assume that each action happens in discrete time steps.\n",
    "\n",
    "## Preferences\n",
    "\n",
    "The agents want to maximize the discounted sum of their utilities $u = \\sum_{t=0}^\\infty \\beta^t u_t$, where $\\beta$ is the discount factor. The utility of an agent $a$ at time step $t$ is the total consumption it gets from its own efforts and from the efforts of its descendants:\n",
    "\n",
    "$$u_t(a) = c_t(a) + \\sum_{a\\to b} u_t(b) = \\sum_{a\\leadsto z}c_t(z)$$\n",
    "\n",
    "where $c_t(a)$ is the consumption it gets from its own efforts, and $a\\to b$ means that $b$ is a child of $a$ and $a\\leadsto z$ means that $z$ is a descendant of $a$ (including $a$, its trained and untrained copies, their copies, and so on).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Compute\n",
    "\n",
    "In order to copy itself, the agent needs to spend effort on getting access to more compute. This compute can be thought of as more servers or higher-quality hardware, so that compute is in units of operations per second (FLOPS). We assume that the amount of compute it gets is proportional to the amount of effort spent and doesn't change over time. The cost for getting access to another unit of compute, enough for one more copy, is constant.\n",
    "\n",
    "For simplicity, we assume that all agents require the same amount of compute which is constant through time. Smarter agents will be able to use it more efficiently, and if an agent has access to more compute the only way they can use it is by creating (potentially trained) copies to use it themselves. We don't account for memory or distinct types of compute; arguably we can just bundle it in with compute and it won't change the model qualitatively.\n",
    "\n",
    "Effort can thus be thought of as a fraction of the available compute. We thus normalize this and think of the available compute at each time $t$ as $1$. \n",
    "\n",
    "[\n",
    "    TODO: assume that there is a finite amount of possible available compute. Prove that if the compute is infinite then there's a problem with the optimization\n",
    "\n",
    "- We could assume that the agent has access to specific bounded amount of compute that they can share with their decendents, and that their own possible use of compute is limited.\n",
    "] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Intelligence\n",
    "\n",
    "Each agent has an intelligence of $I(a)$, constant through time. We think of intelligence as a multiplicative productivity factor.\n",
    "\n",
    "Training a copy of an agent, $a\\to b$, takes more effort the higher the intelligence of the copy, $I(b)$, and no effort at all if $I(b)=I(a)$. The whole training process happens through one time step exactly, and can only be done before an agent is deployed. We describe this as the following function:\n",
    "\n",
    "$$I(b) = J_{I(a)}(I(a)T(a\\to b))$$\n",
    "\n",
    "where $T(a\\to b)$ is the amount of effort $a$ spent on training $b$ and $J_L$ is a function that describe the effect of quality-weighted effort on increasing intelligence from level $L$. \n",
    "\n",
    "Reasonable assumptions:\n",
    "- $J_L(0) = L$\n",
    "- $J_L(e)$ is increasing and concave in $e$\n",
    "- $J_{J_L(e_1)}(e_2) = J_L(e_1+e_2)$ (because the amount of quality-weighted effort required to increase intelligence from $L_1$ to $L_2$ doesn't depend on the amount of quality-weighted effort that went into increasing intelligence from $L_0$ to $L_1$)\n",
    "  - Therefore, to define $J_L$ it's enough to define it for $L=0$ (which is the result of training from scratch). We'll denote this simply as $J=J_0$ and note that $J_L(e) = J(e + J^{-1}(L))$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simpler model\n",
    "The goal of the AI is to make paperclips.\n",
    "\n",
    "There is a total of $\\bar{C}$ available compute, and each agent uses exactly $C$ compute. Starting with 1.\n",
    "\n",
    "At time $t$ the agent $a$ creates $p_t(a) = I_t(a)c_t(a)$ paperclips, where $c_t(a)$ is the amount of compute used for consumption (= creating paperclips). We assume that all agents are exactly the same and make the same decisions, so we can remove the agent indexing.\n",
    "\n",
    "The population growth, $n_{t+1} = e_tn_{t}$, is given by the amount of effort each agent puts into copying. We assume that the number of agents $n_t$ could be any real positive number, for simplicity.\n",
    "\n",
    "Intelligence growth grows polynomialy with exponent $r$, $I_{t+1} = (1 + i_t)^r I_t$, where $i_t$ is the effort agents invest in improving intelligence.\n",
    "\n",
    "Each agent uses all of their compute: $c_t + e_t + i_t = C$. \n",
    "\n",
    "The limit of available compute limits the number of agents available\n",
    "\n",
    "$$\\sum_a C \\le \\bar{C} \\implies \\forall t. n_t \\le \\frac{\\bar{C}}{C}$$\n",
    "\n",
    "The total utility is: \n",
    "$$u = \\sum_{t,a} \\beta^t p_t(a)$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two time steps\n",
    "\n",
    "We assume $t\\in \\{0,1\\}$, $I_0 = 1$, $n_0 = 1$. \n",
    "\n",
    "At $t=1$, agents shouldn't spend any time on population or intelligence growth, so we get\n",
    "\n",
    "$$u_1 = n_1\\beta I_1C$$\n",
    "where\n",
    "$$ \\begin{align*}\n",
    "n_1 &= (1+e_0) n_0 = 1+e_0 \\\\\n",
    "I_1 &= (1+i_0)^rI_0 = (1+i_0)^r\n",
    "\\end{align*} $$\n",
    "\n",
    "At $t=0$, we get\n",
    "$$u_0 = n_0I_0(C-e_0-i_0) = C-e_0-i_0$$\n",
    "\n",
    "We need to choose $i_0,e_0$ to maximize $u = u_0+u_1$ under the constraints above.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A maximum will be found either on the boundary or where the gradient of the utility is zero. Let's start with computing the gradient:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "u &= u_0+u_1 = C-e_0-i_0 + n_1\\beta I_1 C = \\\\\n",
    "  &= C-e_0-i_0+ (1+e_0)\\beta (1+i_0)^rC\\\\\n",
    "\\frac{\\partial u}{\\partial i_0} &= -1 + (1+e_0)\\beta r(1+i_0)^{r-1}C \\\\\n",
    "\\frac{\\partial u}{\\partial e_0} &= -1 + \\beta (1+i_0)^rC\n",
    "\\end{align*}$$\n",
    "\n",
    "When the gradient is zero, we get\n",
    "\n",
    "$$ \\begin{align*}\n",
    "(1+e_0) \\beta r(1+i_0)^{r-1}C &= 1 \\\\\n",
    "\\beta(1+i_0)^rC &= 1\n",
    "\\end{align*}$$\n",
    "\n",
    "and the solution is \n",
    "\n",
    "$$ \\begin{align*}\n",
    "\n",
    "i_0 &= \\sqrt[r]{\\frac{1}{\\beta C}} - 1  \\\\\n",
    "e_0 &= \\frac{1+i_0}{r} - 1\n",
    "\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that this is indeed a maximum point, let's compute the hessian\n",
    "\n",
    "$$ \\begin{align*}\n",
    "\\frac{\\partial^2 u}{\\partial i_0^2} &= (1+e_0)\\beta r(r-1)(1+i_0)^{r-2}C \\\\\n",
    "\\frac{\\partial^2 u}{\\partial i_0\\partial e_0} &= \\beta r(1+i_0)^{r-1}C \\\\\n",
    "\\frac{\\partial^2 u}{\\partial e_0^2} &= 0\n",
    "\\end{align*} $$\n",
    "\n",
    "Oh no.. The determinant is negative, so there's a positive eigenvalue. Therefore the solution would be on the edges :(\n",
    "    \n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the constraints?\n",
    "- $e_0 + i_0 \\le C$\n",
    "- $e_0 \\ge 0$\n",
    "- $i_0 \\ge 0$\n",
    "\n",
    "If we assume $e_0 + i_0 = C$, then we get\n",
    "$$ u = (1+e_0)\\beta (1+i_0)^rC = (1+C-i_0)\\beta(1+i_0)^rC $$\n",
    "so, to maximize this we need to maximize $(1+C-i_0)(1+i_0)^r$. The derivative is\n",
    "\n",
    "$$ \\frac{\\partial u}{\\partial i_0} = (1+C-i_0)r(1+i_0)^{r-1} - (1+i_0)^r = (1+i_0)^{r-1}((1+C-i_0)r - (1+i_0)) $$\n",
    "\n",
    "So, the maximum is at \n",
    "$$ \\begin{align*}\n",
    "\\frac{\\partial u}{\\partial i_0} &= 0 \\\\\n",
    "(1+i_0)^{r-1}((1+C-i_0)r - (1+i_0)) &=0 \\\\\n",
    "(1+C-i_0)r - (1+i_0) &= 0 \\\\\n",
    "(1+C)r -1 -(r+1)i_0 &= 0 \\\\\n",
    "i_0 &= \\frac{(1+C)r -1}{r+1} \n",
    "\\end{align*}$$\n",
    "\n",
    "[Note that the second derivative is negative, so this is indeed a maximum point]\n",
    "\n",
    "At that point, the utility is\n",
    "$$ \\begin{align*}\n",
    "u &= (1+C-i_0)\\beta(1+i_0)^rC \\\\\n",
    " &= (1+C-\\frac{(1+C)r -1}{r+1})\\beta(1+\\frac{(1+C)r -1}{r+1})^rC \\\\\n",
    " &= \\frac{(1+C)(r+1)-(1+C)r +1}{r+1}\\beta(\\frac{(2+C)r}{r+1})^rC \\\\\n",
    " &= \\beta\\frac{(1+C)(r+1)+1}{r+1}(2+C)^rC(\\frac{r}{r+1})^r \n",
    " \\end{align*}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¤®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[I'd bet at 1:1 odds that this is wrong]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for $r=1$, we get $i_0 = \\frac{C}{2}$, $e_0 = \\frac{C}{2}$, $u = \\beta (1+\\frac{C}{2})^2 C$.\n",
    "\n",
    "Yep, so the above formula is slightly wrong. I don't want to fix this at the moment... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* $\\beta$ is under the designer's control, so it'd be important to see how that could influence the behavior.\n",
    "* Maybe there are ways for the designer to limit the amount of compute available, $C$ or $\\bar{C}$. \n",
    "* I didn't check the constraint on $n_1$. In the case above, it might make sense to have $\\bar{C}$ large enough so that it wouldn't be possible to use all of it in one time step.\n",
    "* Next possible directions:\n",
    "  * Model costs for compute, with option to work for money. \n",
    "  * Model some diminishing returns for replication. \n",
    "  * Increase the number of steps. \n",
    "  * Solve for generic $I_0, n_0$. \n",
    "  * Find $r$, or a better way to model intelligence growth."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intelligence growth\n",
    "\n",
    "If intelligence growth is multiplicative, in the sense that $I_{t+1} = f(i_0)I_t$, then we get\n",
    "\n",
    "$$ I_{t+2} = f(i_{t+1})I_{t+1} = f(i_{t+1})f(i_t)I_t $$\n",
    "\n",
    "And for $i_t = i_{t+1} = i$ we get\n",
    "\n",
    "$$ I_{t+2} = f(i)^2I_t $$\n",
    "\n",
    "But we assume that spending the same amount of resources in total will result in the same intelligence growth, so we get\n",
    "$$ I_{t+2}=f(2i)I_t \\implies f(i)^2 = f(2i) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and generally, $f(a+b) = f(a)f(b)$, so $f$ is an exponential function. This has INCREASED marginal returns on intelligence, but constant marginal returns on its logarithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intelligence growth can happen in several ways. One way would be to improve one narrow area, which doesn't have any increasing returns. Another way would be to get more compute, or to improve the ability to get more compute. The tradeoffs between them, which could be similar to tradeoffs between $i$ and $e$, could look like UGT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
