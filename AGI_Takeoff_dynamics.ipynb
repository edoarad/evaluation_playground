{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGI Takeoff dynamics - Intelligence vs Quantity explosion\n",
    "## Summary\n",
    "In Galor & Weil's Unified Growth Theory (UGT), a model is given to explain the drivers for economic growth throughout history. The main insight of the model is that until relatively recently it was more worthwhile to have more kids than to invest in their education. This is because the marginal product of labor was low, and the marginal product of education was even lower. However, as technology advanced, the marginal product of labor increased, and the marginal product of education increased even more. This led to a transition from a Malthusian equilibrium to a modern growth equilibrium that's dominated more by technological growth.\n",
    "\n",
    "We explore how this model could be extended to explain AGI takeoff dynamics. Our key question is whether we expect AGI to gain more by investing in improving its intelligence or by copying itself many times. \n",
    "\n",
    "## Model overview\n",
    "We assume that all AI agents have a common goal and make similar decisions, but we don't account for any coordination between them. Their utility is given by a time-discounted variable that depends on efficiency-weighted effort they put directly into it, so we think of it as \"consumption\". They care about their own consumption, but also about the consumption of their descendants.\n",
    "\n",
    "The efficiency of an AI agent is given by its intelligence, which is a function of the amount of effort that went into its training. \n",
    "\n",
    "The resources they can spend are all compute-time, which itself can grow if they put effort into it.\n",
    "\n",
    "When an agent gets access to more compute, it can use it by copying itself and training the copies. They can choose to put no effort into training, in which case the copies will be identical to the original. [TODO maybe it'd be easier if the agent can train itself, rather than train its decendents?]\n",
    "\n",
    "We also assume that each action happens in discrete time steps.\n",
    "\n",
    "## Preferences\n",
    "\n",
    "The agents want to maximize the discounted sum of their utilities $u = \\sum_{t=0}^\\infty \\beta^t u_t$, where $\\beta$ is the discount factor. The utility of an agent $a$ at time step $t$ is the total consumption it gets from its own efforts and from the efforts of its descendants:\n",
    "\n",
    "$$u_t(a) = c_t(a) + \\sum_{a\\to b} u_t(b) = \\sum_{a\\leadsto z}c_t(z)$$\n",
    "\n",
    "where $c_t(a)$ is the consumption it gets from its own efforts, and $a\\to b$ means that $b$ is a child of $a$ and $a\\leadsto z$ means that $z$ is a descendant of $a$ (including $a$, its trained and untrained copies, their copies, and so on).\n",
    "\n",
    "## Compute\n",
    "\n",
    "In order to copy itself, the agent needs to spend effort on getting access to more compute. This compute can be thought of as more servers or higher-quality hardware, so that compute is in units of operations per second (FLOPS). We assume that the amount of compute it gets is proportional to the amount of effort spent and doesn't change over time. The cost for getting access to another unit of compute, enough for one more copy, is constant.\n",
    "\n",
    "For simplicity, we assume that all agents require the same amount of compute which is constant through time. Smarter agents will be able to use it more efficiently, and if an agent has access to more compute the only way they can use it is by creating (potentially trained) copies to use it themselves. We don't account for memory or distinct types of compute; arguably we can just bundle it in with compute and it won't change the model qualitatively.\n",
    "\n",
    "Effort can thus be thought of as a fraction of the available compute. We thus normalize this and think of the available compute at each time $t$ as $1$. \n",
    "\n",
    "## Intelligence\n",
    "\n",
    "Each agent has an intelligence of $I(a)$, constant through time. We think of intelligence as a multiplicative productivity factor.\n",
    "\n",
    "Training a copy of an agent, $a\\to b$, takes more effort the higher the intelligence of the copy, $I(b)$, and no effort at all if $I(b)=I(a)$. The whole training process happens through one time step exactly, and can only be done before an agent is deployed. We describe this as the following function:\n",
    "\n",
    "$$I(b) = J_{I(a)}(I(a)T(a\\to b))$$\n",
    "\n",
    "where $T(a\\to b)$ is the amount of effort $a$ spent on training $b$ and $J_L$ is a function that describe the effect of quality-weighted effort on increasing intelligence from level $L$. \n",
    "\n",
    "Reasonable assumptions:\n",
    "- $J_L(0) = L$\n",
    "- $J_L(e)$ is increasing and concave in $e$\n",
    "- $J_{J_L(e_1)}(e_2) = J_L(e_1+e_2)$ (because the amount of quality-weighted effort required to increase intelligence from $L_1$ to $L_2$ doesn't depend on the amount of quality-weighted effort that went into increasing intelligence from $L_0$ to $L_1$)\n",
    "  - Therefore, to define $J_L$ it's enough to define it for $L=0$ (which is the result of training from scratch). We'll denote this simply as $J=J_0$ and note that $J_L(e) = J(e + J^{-1}(L))$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
